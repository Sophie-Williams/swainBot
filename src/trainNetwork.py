import numpy as np
import tensorflow as tf
import random

from cassiopeia import riotapi
from draftstate import DraftState
import championinfo as cinfo
import matchProcessing as mp
import experienceReplay as er
from rewards import getReward

from copy import deepcopy
import sqlite3
import draftDbOps as dbo


def trainNetwork(Qnet, numEpochs, numEpisodes, batchSize, bufferSize, loadModel):
    """
    Args:
        Qnet (qNetwork): Q-network to be trained.
        numEpochs (int): number of times to learn on given data
        numEpisodes (int): number of drafts to be simulated per epoch
        batchSize (int): size of each training set sampled from the replay buffer which will be used to update Qnet at a time
        bufferSize (int): size of replay buffer used
        loadModel (bool): flag to reload existing model
    Returns:
        None
    Trains the Q-network Qnet in batches using experience replays.
    """
    print("***")
    print("Beginning training..")
    print("  numEpochs: {}".format(numEpochs))
    print("  numEpisodes: {}".format(numEpisodes))
    print("  batchSize: {}".format(batchSize))
    print("  bufferSize: {}".format(bufferSize))

    # Number of steps to take before doing any training. Needs to be at least batchSize to avoid error when sampling from experience replay
    preTrainingSteps = batchSize
    # Number of steps to take between training
    updateFreq = 10 # 5 picks + 5 bans per match -> train after every match (hyperparameter)
    # We can't validate a winner for matches with submissions generated by the learner,
    # we will use a winner-less match when getting rewards for states that
    blankMatch = {"winner":None}
    # Start training
    with tf.Session() as sess:
        # Open saved model (if flagged)
        if loadModel:
            Qnet.saver.restore(sess,"tmp/model.ckpt")
        else:
            # Otherwise, initialize tensorflow variables
            sess.run(Qnet.init)
        for i in range(numEpochs):
            print("Beginning epoch {}".format(i))
            epsilon = 1 # Probability of letting the learner submit its own action
            # Initialize experience replay buffer
            experienceReplay = er.ExperienceBuffer(bufferSize)
            # Queue of competitive games from db.
            matchQueue = mp.buildMatchQueue(numEpisodes)
            totalSteps = 0

            for episode in range(numEpisodes):
                # Get next match from queue
                match = matchQueue.get()

                team = DraftState.RED_TEAM if match["winner"]==1 else DraftState.BLUE_TEAM # For now only learn from winning team
                # Process this match into individual experiences
                experiences = mp.processMatch(match, team)
                for experience in experiences:
                    if(random.random() < epsilon):
                        # Let the network predict the next action, if the action leads
                        # to an invalid state replace the original experience with the
                        # negatively reinforced result. Ideally we would like to let the network
                        # predict a random action and evaluate the reward for the resulting state,
                        # but it's not clear how to generate a reward for an action which was not
                        # produced by geniune match data.
                        state = experience[0]
                        a = sess.run(Qnet.prediction,
                                    feed_dict={Qnet.input:[state.formatState()]})
                        (cid,pos) = state.formatAction(a[0])
                        nextState = deepcopy(state)
                        nextState.updateState(cid,pos)
                        if nextState.evaluateState in DraftState.invalidStates:
                            r = getReward(nextState, blankMatch)
                            experience = (state, a, r, nextState)
                    experienceReplay.store([experience])
                    totalSteps += 1

                    # Every updateFreq steps we train the network using the replay buffer
                    if (totalSteps >= preTrainingSteps) and (totalSteps % updateFreq == 0):
                        trainingBatch = experienceReplay.sample(batchSize)

                        #TODO (Devin): Every reference to trainingBatch involves vstacking each column of the batch before using it.. probably better to just have er.sample() return
                        # a numpy array.

                        # Calculate target Q values for each example:
                        # For non-temrinal states, targetQ is estimated according to
                        #   targetQ = r + gamma*max_{a} Q(s',a).
                        # For terminating states (where state.evaluateState() == DS.DRAFT_COMPLETE) the target is computed as
                        #   targetQ = r
                        # therefore, it isn't necessary to feed these states through the ANN to evalutate targetQ. This should save
                        # time as the network complexity increases.
                        updates = []
                        for exp in trainingBatch:
                            startState,_,reward,endingState = exp
                            if endingState.evaluateState() == DraftState.DRAFT_COMPLETE: # Action moves to terminal state
                                updates.append(reward)
                            else:
                                # Each row in predictedQ gives estimated Q(s',a) values for each possible action for the input state s'.
                                predictedQ = sess.run(Qnet.outQ,
                                                      feed_dict={Qnet.input:[endingState.formatState()]})

                                # To get max_{a} Q(s',a) values take max along *rows* of predictedQ.
                                maxQ = np.max(predictedQ,axis=1)[0]
                                updates.append(reward + Qnet.discountFactor*maxQ)
                        targetQ = np.array(updates)
                        # Make sure targetQ shape is correct (sometimes np.array likes to return array of shape (batchSize,1))
                        targetQ.shape = (batchSize,)
                        # Uncomment this section to get a live look at valuation update
    #                    estQ = sess.run(Qnet.outQ,
    #                                    feed_dict={Qnet.input:np.vstack([exp[0].formatState() for exp in trainingBatch])})
    #                    print("Current estimates for Q(s,-) for this experience's initial state..")
    #                    print("a \t \t Q(s,a)")
    #                    print("************************")
    #                    for i in range(estQ.shape[1]):
    #                        print("{} \t \t {}".format(i,estQ[0,i]))

                        # Update Qnet using target Q
                        # Experience replay stores action = (champion_id, position) pairs
                        # these need to be converted into the corresponding index of the input vector to the Qnet
                        actions = np.array([startState.getAction(exp[1][0],exp[1][1]) for exp in trainingBatch])
                        _ = sess.run(Qnet.updateModel,
                                     feed_dict={Qnet.input:np.vstack([exp[0].formatState() for exp in trainingBatch]),
                                                Qnet.actions:actions,
                                                Qnet.target:targetQ})
                if(epsilon > 0.1):
                    # Reduce chance of random actions over time
                    epsilon -= 1./numEpisodes
        # Once training is complete, save the updated network
        outPath = Qnet.saver.save(sess,"tmp/model.ckpt")
        print("qNet model is saved in file: {}".format(outPath))
    print("***")
    return None
