import numpy as np
import tensorflow as tf
import random
import time

from cassiopeia import riotapi
from draftstate import DraftState
import championinfo as cinfo
import matchProcessing as mp
import experienceReplay as er
from rewards import getReward

from copy import deepcopy
import sqlite3
import draftDbOps as dbo


def trainNetwork(Qnet, training_matches, validation_matches, num_epochs, batch_size, buffer_size, load_model = False, verbose = False):
    """
    Args:
        Qnet (qNetwork): Q-network to be trained.
        training_matches (list(match)): list of matches to be trained on
        validation_matches (list(match)): list of matches to validate model against
        num_epochs (int): number of times to learn on given data
        batch_size (int): size of each training set sampled from the replay buffer which will be used to update Qnet at a time
        buffer_size (int): size of replay buffer used
        load_model (bool): flag to reload existing model
        verbose (bool): flag for enhanced output
    Returns:
        (loss,validation_accuracy) tuple
    Trains the Q-network Qnet in batches using experience replays.
    """
    num_episodes = len(training_matches)
    if(verbose):
        print("***")
        print("Beginning training..")
        print("  num_epochs: {}".format(num_epochs))
        print("  num_episodes: {}".format(num_episodes))
        print("  batch_size: {}".format(batch_size))
        print("  buffer_size: {}".format(buffer_size))

    # Number of steps to take before doing any training. Needs to be at least batch_size to avoid error when sampling from experience replay
    pre_training_steps = batch_size
    # Number of steps to take between training
    update_freq = 10 # 5 picks + 5 bans per match
    # We can't validate a winner for submissions generated by the learner,
    # we will use a winner-less match when getting rewards for such states
    blank_match = {"winner":None}
    loss_over_epochs = []
    # Start training
    with tf.Session() as sess:
        # Open saved model (if flagged)
        if load_model:
            Qnet.saver.restore(sess,"tmp/model.ckpt")
            print("Checkpoint loaded..")
        else:
            # Otherwise, initialize tensorflow variables
            sess.run(Qnet.init)
        for i in range(num_epochs):
            t0 = time.time()
            epsilon = 0. # Probability of letting the learner submit its own action
            # Initialize experience replay buffer
            experience_replay = er.ExperienceBuffer(buffer_size)
            # Queue of competitive games from db.
            total_steps = 0
            bad_state_counts = {DraftState.BAN_AND_SUBMISSION:0,
                                DraftState.DUPLICATE_SUBMISSION:0,
                                DraftState.DUPLICATE_ROLE:0,
                                DraftState.INVALID_SUBMISSION:0}
            null_action_count = 0
            for match in training_matches:
                #if random.random() < 0.5:
                #    team = DraftState.BLUE_TEAM
                #else:
                #    team = DraftState.RED_TEAM

                # Only learns from winning team
                team = DraftState.RED_TEAM if match["winner"]==1 else DraftState.BLUE_TEAM
                # Process this match into individual experiences
                experiences = mp.processMatch(match, team)
                for experience in experiences:
                    # Some experiences include NULL submissions (exclusively bans)
                    # We don't allow the learner to submit NULL picks so skip adding these
                    # to the replay buffer.
                    state,a,rew,_ = experience
                    (cid,pos) = a

                    if cid is None:
                        null_action_count += 1
                        continue
                    if(True):#if(random.random() < epsilon):
                        # Let the network predict the next action, if the action leads
                        # to an invalid state add a negatively reinforced experience to the replay buffer.
                        # It is more important to learn to make legal predictions first before learning pick/ban structure.
                        # Ideally we would like to let the network
                        # predict a random action and evaluate the reward for the resulting state,
                        # but it's not clear how to generate a reward for an action which was not
                        # produced by geniune match data.
                        a = sess.run(Qnet.prediction,
                                    feed_dict={Qnet.input:[state.formatState()]})
                        (cid,pos) = state.formatAction(a[0])
                        nextState = deepcopy(state)
                        nextState.updateState(cid,pos)
                        state_code = nextState.evaluateState()
                        if state_code in DraftState.invalid_states:
                            bad_state_counts[state_code] += 1
                            r = getReward(nextState, blank_match)
                            negative_experience = (state, state.formatAction(a[0]), r, nextState)
                            experience_replay.store([negative_experience])
                    experience_replay.store([experience])
                    total_steps += 1

                    # Every update_freq steps we train the network using the replay buffer
                    if (total_steps >= pre_training_steps) and (total_steps % update_freq == 0):
                        training_batch = experience_replay.sample(batch_size)

                        #TODO (Devin): Every reference to training_batch involves stacking each input of the batch before using it.. probably better to just have er.sample() return
                        # a numpy array.

                        # Calculate target Q values for each example:
                        # For non-temrinal states, targetQ is estimated according to
                        #   targetQ = r + gamma*max_{a} Q(s',a).
                        # For terminating states (where state.evaluateState() == DS.DRAFT_COMPLETE) the target is computed as
                        #   targetQ = r
                        # therefore, it isn't necessary to feed these states through the ANN to evalutate targetQ. This should save
                        # time as the network complexity increases.
                        updates = []
                        for exp in training_batch:
                            startState,_,reward,endingState = exp
                            if endingState.evaluateState() == DraftState.DRAFT_COMPLETE: # Action moves to terminal state
                                updates.append(reward)
                            else:
                                # Each row in predictedQ gives estimated Q(s',a) values for each possible action for the input state s'.
                                predictedQ = sess.run(Qnet.outQ,
                                                      feed_dict={Qnet.input:[endingState.formatState()]})
                                # To get max_{a} Q(s',a) values take max along *rows* of predictedQ.
                                maxQ = np.max(predictedQ,axis=1)[0]
                                updates.append(reward + Qnet.discount_factor*maxQ)
                        targetQ = np.array(updates)
                        # Make sure targetQ shape is correct (sometimes np.array likes to return array of shape (batch_size,1))
                        targetQ.shape = (batch_size,)

                        # Update Qnet using target Q
                        # Experience replay stores action = (champion_id, position) pairs
                        # these need to be converted into the corresponding index of the input vector to the Qnet
                        actions = np.array([startState.getAction(exp[1][0],exp[1][1]) for exp in training_batch])
                        _ = sess.run(Qnet.updateModel,
                                     feed_dict={Qnet.input:np.stack([exp[0].formatState() for exp in training_batch],axis=0),
                                                Qnet.actions:actions,
                                                Qnet.target:targetQ})
                if(epsilon > 0.1):
                    # Reduce chance of random actions over time
                    epsilon -= 1./num_episodes

            t1 = time.time()-t0
            loss,val_accuracy = validate_model(sess, training_matches, Qnet)
            loss_over_epochs.append(loss)
            # Once training is complete, save the updated network
            out_path = Qnet.saver.save(sess,"tmp/model.ckpt")
            if(verbose):
                print(" Finished epoch {}/{}: dt {:.2f}, mem {}, loss {:.6f}, train {:.6f}".format(i+1,num_epochs,t1,total_steps+null_action_count,loss,val_accuracy))
                invalid_action_count = sum([bad_state_counts[k] for k in bad_state_counts])
                print("  negative memories added = {}".format(invalid_action_count))
                print("  bad state distributions:")
                for code in bad_state_counts:
                    print("   {} -> {} counts".format(code,bad_state_counts[code]))

                print("qNet model is saved in file: {}".format(out_path))
                print("***")

    stats = (loss_over_epochs,val_accuracy)
    return stats

def validate_model(sess, validation_data, Qnet):
    """
    Validates given model by computing loss and absolute accuracy for validation data using current Qnet estimates.
    Args:
        sess (tensorflow Session): TF Session to run model in
        validation_data (list(matchIds)): list of match ids to validate against
        Qnet (qNetwork): tensorflow q network of model to validate
    Returns:
        stats (tuple(float)): list of statistical measures of performance. stats = (loss,acc)
    """
    val_replay = er.ExperienceBuffer(10*len(validation_data))
    for match in validation_data:
        team = DraftState.RED_TEAM if match["winner"]==1 else DraftState.BLUE_TEAM
        # Process this match into individual experiences
        experiences = mp.processMatch(match, team)
        for exp in experiences:
            val_replay.store([exp])
    n_experiences = val_replay.getBufferSize()
    val_experiences = val_replay.sample(n_experiences)
    state,_,_,_ = val_experiences[0]
    val_states = np.zeros((n_experiences,)+state.formatState().shape)
    val_actions = np.zeros((n_experiences,))
    val_targets = np.zeros((n_experiences,))
    for n in range(n_experiences):
        start,act,rew,finish = val_experiences[n]
        val_states[n,:,:] = start.formatState()
        (cid,pos) = act
        # Skip null actions such as missing/skipped bans
        if cid is None:
            continue
        val_actions[n] = start.getAction(cid,pos)
        if finish.evaluateState() == DraftState.DRAFT_COMPLETE:
            # Action moves to terminal state
            val_targets[n] = rew
        else:
            # Each row in predictedQ gives estimated Q(s',a) values for each possible action for the input state s'.
            predicted_Q = sess.run(Qnet.outQ,
                                  feed_dict={Qnet.input:[finish.formatState()]})
            # To get max_{a} Q(s',a) values take max along *rows* of predictedQ.
            max_Q = np.max(predicted_Q,axis=1)[0]
            val_targets[n] = (rew + Qnet.discount_factor*max_Q)

    loss,pred_actions = sess.run([Qnet.loss, Qnet.prediction],
                    feed_dict={Qnet.input:val_states,
                               Qnet.actions:val_actions,
                               Qnet.target:val_targets})
#            print(" t:{}".format([int(i) for i in train_actions]))
#            print(" p:{}".format([int(i) for i in pred_actions]))
    val_accuracy = (n_experiences-np.count_nonzero(val_actions-pred_actions))/n_experiences
    return (loss, val_accuracy)
